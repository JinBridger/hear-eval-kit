import argparse
import json
import os
import torch
import torchaudio
import numpy as np
from pathlib import Path
from typing import Optional, Union, List
from tqdm import tqdm

from stable_audio_tools.models.factory import create_model_from_config
from stable_audio_tools.models.utils import load_ckpt_state_dict

def set_seed(seed=42):
    """è®¾ç½®æ‰€æœ‰éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§"""
    import random
    
    random.seed(seed)
    torch.manual_seed(seed)
    np.random.seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
def set_h20():
    print("\033[1;92mâŒ Using H20, disabling TF32\033[0m")
    torch.set_float32_matmul_precision('highest')
    torch.backends.cuda.matmul.allow_tf32 = False
    torch.backends.cudnn.allow_tf32 = False

class AudioAutoencoderInference:
    """éŸ³é¢‘è‡ªç¼–ç å™¨æ¨ç†ç±»"""
    
    def __init__(
        self,
        model_config_path: str,
        checkpoint_path: str,
        device: str = "cuda" if torch.cuda.is_available() else "cpu",
        use_half: bool = False,
    ):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            model_config_path: æ¨¡å‹é…ç½®æ–‡ä»¶è·¯å¾„ (JSON)
            checkpoint_path: æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„
            device: è¿è¡Œè®¾å¤‡ ("cuda" æˆ– "cpu")
            use_half: æ˜¯å¦ä½¿ç”¨åŠç²¾åº¦ (FP16)
        """
        self.device = device
        self.use_half = use_half
        
        # åŠ è½½é…ç½®
        print(f"ğŸ“„ åŠ è½½æ¨¡å‹é…ç½®: {model_config_path}")
        with open(model_config_path) as f:
            self.model_config = json.load(f)
        
        self.sample_rate = self.model_config.get("sample_rate", 48000)
        self.model_type = self.model_config.get("model_type", "autoencoder")
        
        # åˆ›å»ºæ¨¡å‹
        print(f"ğŸ—ï¸  åˆ›å»ºæ¨¡å‹: {self.model_type}")
        self.model = create_model_from_config(self.model_config)
        
        # åŠ è½½æƒé‡
        print(f"ğŸ“¦ åŠ è½½æƒé‡: {checkpoint_path}")
        state_dict = load_ckpt_state_dict(checkpoint_path)
        
        # å¤„ç†å¯èƒ½çš„åŒ…è£…å±‚ (å¦‚ Lightning çš„ state_dict)
        if "state_dict" in state_dict:
            state_dict = state_dict["state_dict"]
        
        # ç§»é™¤å¯èƒ½çš„å‰ç¼€ (å¦‚ "model." æˆ– "autoencoder.")
        new_state_dict = {}
        for k, v in state_dict.items():
            new_key = k
            for prefix in ["model.", "autoencoder.", "pretransform."]:
                if k.startswith(prefix):
                    new_key = k[len(prefix):]
                    break
            new_state_dict[new_key] = v
        
        self.model.load_state_dict(new_state_dict, strict=False)
        
        # ç§»åˆ°è®¾å¤‡å¹¶è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        self.model = self.model.to(device)
        if use_half:
            self.model = self.model.half()
        self.model.eval()
        
        # æ£€æŸ¥æ˜¯å¦æ”¯æŒæ–‡æœ¬æ¡ä»¶
        self.is_text_conditioned = hasattr(self.model, 'encode_text')
        
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ!")
        print(f"   è®¾å¤‡: {device}")
        print(f"   é‡‡æ ·ç‡: {self.sample_rate} Hz")
        print(f"   æ–‡æœ¬æ¡ä»¶: {'æ”¯æŒ' if self.is_text_conditioned else 'ä¸æ”¯æŒ'}")
        print(f"   ç²¾åº¦: {'FP16' if use_half else 'FP32'}")
        
        self.audio_channels = self.model_config.get("audio_channels", 2)

    def load_audio(self, audio_path: str) -> torch.Tensor:
        """
        åŠ è½½éŸ³é¢‘æ–‡ä»¶å¹¶é¢„å¤„ç†
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            audio: [1, channels, samples] çš„å¼ é‡
        """
        audio, sr = torchaudio.load(audio_path)
        
        # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if sr != self.sample_rate:
            resampler = torchaudio.transforms.Resample(sr, self.sample_rate)
            audio = resampler(audio)
        
        # ç¡®ä¿æ˜¯åŒå£°é“
        audio_channels = self.audio_channels
        if audio.shape[0] < audio_channels:
            # å•å£°é“è½¬ç«‹ä½“å£°
            audio = audio.repeat(audio_channels, 1)
        elif audio.shape[0] > audio_channels:
            # å¤šå£°é“è½¬ç›®æ ‡å£°é“æ•°
            audio = audio[:audio_channels]
        
        # æ·»åŠ batchç»´åº¦: [channels, samples] -> [1, channels, samples]
        audio = audio.unsqueeze(0)
        
        return audio
    
    def save_audio(self, audio: torch.Tensor, output_path: str):
        """
        ä¿å­˜éŸ³é¢‘åˆ°æ–‡ä»¶
        
        Args:
            audio: [1, channels, samples] çš„å¼ é‡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        # ç§»é™¤batchç»´åº¦: [1, channels, samples] -> [channels, samples]
        audio = audio.squeeze(0).cpu()
        
        # è£å‰ªåˆ° [-1, 1] èŒƒå›´
        audio = torch.clamp(audio, -1.0, 1.0)
        
        # ä¿å­˜
        os.makedirs(os.path.dirname(output_path) or ".", exist_ok=True)
        torchaudio.save(output_path, audio, self.sample_rate)
    
    @torch.no_grad()
    def encode(
        self, 
        audio: Union[str, torch.Tensor],
        return_info: bool = False,
    ) -> Union[torch.Tensor, tuple]:
        """
        ç¼–ç éŸ³é¢‘åˆ°æ½œåœ¨ç©ºé—´
        
        Args:
            audio: éŸ³é¢‘è·¯å¾„æˆ–å¼ é‡ [1, channels, samples]
            return_info: æ˜¯å¦è¿”å›é¢å¤–ä¿¡æ¯
            
        Returns:
            latents: æ½œåœ¨è¡¨ç¤º [1, latent_dim, latent_len]
            info: (å¯é€‰) ç¼–ç ä¿¡æ¯å­—å…¸
        """
        # åŠ è½½éŸ³é¢‘
        if isinstance(audio, str):
            audio = self.load_audio(audio)
        
        audio = audio.to(self.device)
        if self.use_half:
            audio = audio.half()
        
        # ç¼–ç 
        result = self.model.encode(audio, return_info=return_info)
        
        if return_info:
            latents, info = result
            return latents, info
        else:
            return result
    
    @torch.no_grad()
    def decode(
        self, 
        latents: torch.Tensor,
        caption: Optional[str] = None,
    ) -> torch.Tensor:
        """
        ä»æ½œåœ¨è¡¨ç¤ºè§£ç éŸ³é¢‘
        
        Args:
            latents: æ½œåœ¨è¡¨ç¤º [1, latent_dim, latent_len]
            caption: (å¯é€‰) æ–‡æœ¬æè¿°ï¼Œç”¨äºæ¡ä»¶è§£ç 
            
        Returns:
            audio: é‡å»ºçš„éŸ³é¢‘ [1, channels, samples]
        """
        latents = latents.to(self.device)
        
        # å¦‚æœæ˜¯æ–‡æœ¬æ¡ä»¶æ¨¡å‹ä¸”æä¾›äº†caption
        if self.is_text_conditioned and caption is not None:
            # ç¼–ç æ–‡æœ¬
            text_embeds, attention_mask = self.model.encode_text([caption])
            
            # æ¡ä»¶è§£ç 
            audio = self.model.decode(
                latents, 
                text_embeds=text_embeds,
                text_attention_mask=attention_mask
            )
        else:
            # æ— æ¡ä»¶è§£ç 
            audio = self.model.decode(latents)
        
        return audio
    
    @torch.no_grad()
    def reconstruct(
        self, 
        audio: Union[str, torch.Tensor],
        caption: Optional[str] = None,
        return_latents: bool = False,
    ) -> Union[torch.Tensor, tuple]:
        """
        é‡å»ºéŸ³é¢‘ (ç¼–ç  + è§£ç )
        
        Args:
            audio: éŸ³é¢‘è·¯å¾„æˆ–å¼ é‡
            caption: (å¯é€‰) æ–‡æœ¬æè¿°
            return_latents: æ˜¯å¦åŒæ—¶è¿”å›æ½œåœ¨è¡¨ç¤º
            
        Returns:
            reconstructed: é‡å»ºçš„éŸ³é¢‘
            latents: (å¯é€‰) æ½œåœ¨è¡¨ç¤º
        """
        # ç¼–ç 
        latents = self.encode(audio)
        
        # è§£ç 
        reconstructed = self.decode(latents, caption=caption)
        
        if return_latents:
            return reconstructed, latents
        else:
            return reconstructed
    
    def inference_single(
        self,
        input_path: str,
        output_path: str,
        caption: Optional[str] = None,
        idx: int = None,
    ):
        """
        å•ä¸ªæ–‡ä»¶æ¨ç†
        
        Args:
            input_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            output_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„
            caption: (å¯é€‰) æ–‡æœ¬æè¿°
        """
           
        # åŠ è½½éŸ³é¢‘
        audio = self.load_audio(input_path)
          
        # é‡å»º
        reconstructed, latents = self.reconstruct(audio, caption=caption, return_latents=True)
        
        
        # ä¿å­˜
        self.save_audio(reconstructed, output_path)
        if idx is not None and idx<5:
            print(f"   ç´¢å¼•: {idx}")
            print(f"ğŸµ å¤„ç†: {input_path}")
            print(f"   åŸå§‹éŸ³é¢‘: {audio.shape}")
            print(f"   æ½œåœ¨è¡¨ç¤º: {latents.shape}")
            print(f"   é‡å»ºéŸ³é¢‘: {reconstructed.shape}")
            print(f"ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
    
    def inference_batch(
        self,
        input_dir: str,
        output_dir: str,
        caption: Optional[str] = None,
        extensions: tuple = ('.wav', '.mp3', '.flac', '.ogg'),
    ):
        """
        æ‰¹é‡æ¨ç†
        
        Args:
            input_dir: è¾“å…¥ç›®å½•
            output_dir: è¾“å‡ºç›®å½•
            caption: (å¯é€‰) ç»Ÿä¸€çš„æ–‡æœ¬æè¿°
            extensions: æ”¯æŒçš„éŸ³é¢‘æ ¼å¼
        """
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æ”¶é›†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        audio_files = []
        for ext in extensions:
            audio_files.extend(input_dir.glob(f"*{ext}"))
        
        print(f"ğŸ“ æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        # æ‰¹é‡å¤„ç†
        for idx, audio_file in enumerate(tqdm(audio_files, desc="æ¨ç†è¿›åº¦")):
            output_file = output_dir / f"{audio_file.stem}.wav"

            self.inference_single(str(audio_file), str(output_file), caption=caption, idx=idx)
            


from diffusers.models.autoencoders.autoencoder_oobleck import AutoencoderOobleck
import torch
from torch import nn
from typing import List
import argparse
import os
import torchaudio
from pathlib import Path
import copy
import torch.nn.functional as F
import numpy as np
from tqdm import tqdm
from typing import Optional, Union, List
from math import pi
import yaml
from safetensors.torch import load_file



class VAEInference(AudioAutoencoderInference):

    def __init__(
        self,
        device="cuda" if torch.cuda.is_available() else "cpu",
        use_half=False,
        sample_rate=44100,
    ):

        self.model = AutoencoderOobleck.from_pretrained(
            "/data/jq_data/stable-audio-open-1.0", subfolder="vae"
        )

        self.model.to(device)
        self.device = device
        self.use_half = use_half
        self.sample_rate = sample_rate
        self.audio_channels = 2


    @torch.no_grad()
    def encode(
        self, 
        audio: Union[str, torch.Tensor],
        return_info: bool = False,
    ) -> Union[torch.Tensor, tuple]:
        """
        Encode audio to latent space
        
        Args:
            audio: Audio path or tensor [1, channels, samples]
            return_info: Whether to return extra info
            
        Returns:
            latents: Latent representation [1, latent_dim, latent_len]
            info: (optional) Encoding info dict
        """
        assert return_info == False, "return_info is not supported"
        # åŠ è½½éŸ³é¢‘
        if isinstance(audio, str):
            audio = self.load_audio(audio)
        
        audio = audio.to(self.device)
        if self.use_half:
            audio = audio.half()
        
        # ç¼–ç 
        result = self.model.encode(audio)
        result = result.latent_dist.sample()

        return result

    @torch.no_grad()
    def decode(
        self, 
        latents: torch.Tensor,
        caption: Optional[str] = None,
    ) -> torch.Tensor:
        """
        Decode audio from latent representation
        
        Args:
            latents: Latent representation [1, latent_dim, latent_len]
            caption: (optional) Text description for conditional decoding
            
        Returns:
            audio: Reconstructed audio [1, channels, samples]
        """
        latents = latents.to(self.device)
        
      
        audio = self.model.decode(latents).sample.cpu()[0]
        
        return audio

        

def main():
    parser = argparse.ArgumentParser(description="Audio autoencoder inference")
    
    # Model parameters
    parser.add_argument("--seed", type=int, default=42, help="Random seed")
    parser.add_argument("--device", type=str, default="cuda" if torch.cuda.is_available() else "cpu",
                        help="Device (cuda/cpu)")
    parser.add_argument("--half", action="store_true",
                        help="Use half precision (FP16)")
    
    # Input/Output
    parser.add_argument("--input", type=str,
                        help="Input audio file path (single file mode)")
    parser.add_argument("--output", type=str,
                        help="Output audio file path (single file mode)")
    parser.add_argument("--input_dir", type=str, default="./input",
                        help="Input directory path (batch mode)")
    parser.add_argument("--output_dir", type=str, default="./output",
                        help="Output directory path (batch mode)")
    
    # Condition parameters
    parser.add_argument("--caption", type=str, default=None,
                        help="Text description (for conditional generation)")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)

    # åˆ›å»ºæ¨ç†å™¨
    inferencer = VAEInference(
        device=args.device,
        use_half=args.half,
    )

    encode_output = inferencer.encode(torch.randn(1, 2, 20*44100), return_info=False)
    encode_output = encode_output.transpose(-1,-2)
    print("Encode output shape:", encode_output.shape)
    
    # # æ‰§è¡Œæ¨ç†
    # if args.input and args.output:
    #     # å•æ–‡ä»¶æ¨¡å¼
    #     inferencer.inference_single(
    #         input_path=args.input,
    #         output_path=args.output,
    #         caption=args.caption,
    #     )
    # elif args.input_dir and args.output_dir:
    #     # æ‰¹é‡æ¨¡å¼
    #     inferencer.inference_batch(
    #         input_dir=args.input_dir,
    #         output_dir=args.output_dir,
    #         caption=args.caption,
    #     )
    # else:
    #     print("Error: Please specify --input/--output or --input_dir/--output_dir")
    #     parser.print_help()




from typing import Tuple

import torch

from torch import Tensor

def load_model(
    model_file_path: str = "", model_hub: str = ""
) -> torch.nn.Module:
    """
    Returns a torch.nn.Module that produces embeddings for audio.

    Args:
        model_file_path: Ignored.
        model_hub: Which wav2vec2 model to load from hugging face.
    Returns:
        Model
    """
    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    # åˆ›å»ºæ¨ç†å™¨
    model = VAEInference(
        device="cuda",
    )

    # sample rate and embedding sizes are required model attributes for the HEAR API
    setattr(model, "sample_rate", 44100)
    setattr(model, "embedding_size", 64)
    setattr(model, "scene_embedding_size", model.embedding_size)
    setattr(model, "timestamp_embedding_size", model.embedding_size)

    return model


def get_timestamp_embeddings(
    audio: Tensor,
    model: torch.nn.Module,
) -> Tuple[Tensor, Tensor]:
    """
    This function returns embeddings at regular intervals centered at timestamps. Both
    the embeddings and corresponding timestamps (in milliseconds) are returned.

    Args:
        audio: n_sounds x n_samples of mono audio in the range [-1, 1].
        model: Loaded model.

    Returns:
        - Tensor: embeddings, A float32 Tensor with shape (n_sounds, n_timestamps,
            model.timestamp_embedding_size).
        - Tensor: timestamps, Centered timestamps in milliseconds corresponding
            to each embedding in the output. Shape: (n_sounds, n_timestamps).
    """

    # Assert audio is of correct shape
    if audio.ndim != 2:
        raise ValueError(
            "audio input tensor must be 2D with shape (n_sounds, num_samples)"
        )

    # Make sure the correct model type was passed in

    # Send the model to the same device that the audio tensor is on.
    # model = model.to(audio.device)

    # Put the model into eval mode, and not computing gradients while in inference.
    # Iterate over all batches and accumulate the embeddings for each frame.
    
    # convert audio to [n_sounds, channels, n_samples] where channels=2
    if audio.ndim == 2:
        audio = audio.unsqueeze(1).repeat(1, 2, 1)  # mono to stereo

    embeddings = model.encode(audio, return_info=False)
    embeddings = embeddings.transpose(-1,-2)

    total_frames = embeddings.shape[1]
    original_length = audio.shape[2] / model.sample_rate * 1000  # in milliseconds


    timestamps = torch.linspace(0, original_length, steps=total_frames + 1).unsqueeze(0)
    # get mid of each frame
    timestamps = (timestamps[:, :-1] + timestamps[:, 1:]) / 2
    
    assert timestamps.shape[1] == embeddings.shape[1]

    return embeddings, timestamps


# TODO: There must be a better way to do scene embeddings,
# e.g. just truncating / padding the audio to 2 seconds
# and concatenating a subset of the embeddings.
def get_scene_embeddings(
    audio: Tensor,
    model: torch.nn.Module,
) -> Tensor:
    """
    This function returns a single embedding for each audio clip. In this baseline
    implementation we simply summarize the temporal embeddings from
    get_timestamp_embeddings() using torch.mean().

    Args:
        audio: n_sounds x n_samples of mono audio in the range [-1, 1]. All sounds in
            a batch will be padded/trimmed to the same length.
        model: Loaded model.

    Returns:
        - embeddings, A float32 Tensor with shape
            (n_sounds, model.scene_embedding_size).
    """
    embeddings, _ = get_timestamp_embeddings(audio, model)
    embeddings = torch.mean(embeddings, dim=1)
    return embeddings